
```{r}
options(scipen = 999)
SEED = 123

library(rethinking)

library(dplyr)
library(ggplot2)
```

# Posterior Approximation

<br>

---

<br>

# Introduction

The posterior $p(X|\theta)$ is generally very difficult to compute directly. Instead, numerical approximation methods are used to estimate the posterior distribution. The 3 most common methods are:

1) Grid approximation (which we previously demonstrated)
2) Quadratic approximation
3) Markov Chain Monte Carlo

<br>

---

<br>

# 2.1 Optimizing Objective Functions In R

Quadratic approximation essentially fits a "paraboloid of best fit" around the log-posterior mode. Thus to do quadratic approximation, we must find:

1) The log-posterior mode, aka the maximum value of the log-posterior
2) The paraboloid of best fit, which is given via a 2nd order Taylor approximation, aka the Hessian matrix.

Both pieces can be obtained using the `optim()` function in the base `stats` package of R. Given a function of the form

$$
f:\mathbb{R}^k \to \mathbb{R}
$$
The `optim()` function will search for the point $(x_1,\ldots,x_k)$ which *minimizes* the function $f$. There a many different search algorithms that can be specified, but the most common one is usually `method = "BFGS"`. The BFGS algorithm stands for Broyden-Fletcher-Goldfarb-Shanno and searches for the minimum using a modified version of gradient descent. Specifically, at each step $k$, it finds the search direction $p_k$ by solving the equation:

$$
B_kp_k = -\nabla f(x_k)$
$$
where $\nabla f(x_k)$ is the gradient at point $x_k$, $B_k$ is the Hessian matrix, and $p_k$ is vector that gives the search direction. Once the search direction is found, a line search is performed to find the minimum point along that direction. Generally speaking, the Hessian matrix is costly to compute directly; the BFGS algorithm is famous for computing the Hessian via updating a previous approximation each step (instead of re-computing the Hessian from scratch every single time).

Here is an example of how use the `optim()` function.

```{r}
# function to optimize: f(x,y) = (x-1)^2 + (y-2)^2 - 10
# this is a paraboloid with a minimum at (x = 1, y = 2)
# Note: the input of the function has to a single vector 
# as the optim() function minimizes the vector
myfunction <- function(vector){
  x <- vector[1]
  y <- vector[2]
  return((x-1)^2 + (y-2)^2 + 10)
}

# find the minimum of myfunction()
solution <- optim(
  par = c(10, -10),  # initial guess
  fn = myfunction,   # functiont minimize
  method = "BFGS",   # search algorithm
  hessian = TRUE     # return the Hessian at the minimum
)

solution
```

The `optim()` function returns a names list. The most important components are `par` which are the input values that give the minimum, `value` which is the minimum value of the function, and `hessian` which stores the Hessian matrix found at the minimum.

<br>

--- 

<br>

# 2.2 Quadratic Approximation

## The Bernstein-von Mises Theorem

Quadratic approximation (aka Gaussian approximation) computes the posterior by declaring that the posterior is approximately a normal distribution. This proclamation isn't arbtirary; under certain regularity conditions (finite-dimensional, well-specified, smooth, existence of tests), if the prior $p(\theta)$ is "smooth enough" then:

$$
\lim_{n\to\infty}p(\theta|x_1,\ldots,x_n) = N(\theta_0, n^{-1}I(\theta_0)^{-1})
$$

where $\theta_0$ is the "true" population parameter and $I(\theta_0)^{-1}$ is the Fisher information matrix (more details in the next paragraph). In other words, as long as our priors are reasonably well-behaved, the posterior will *always converge* to a (multivariate) normal distribution as the number of observations increases. Even more simply put: the posterior becomes more normal as we observe more data. This result is called **Bernstein-von Mises theorem** and is the main justification for quadratic approximation. Intuitively, the Bernstein-von Mises theorem actually makes a lot of sense: the more data we observe, the less dependent we become on the prior distribution. As the body of evidence grows stronger, the posterior distribution begins to focus in on a smaller and smaller region of possible parameter values. The Bernstein-von Mises theorem just says that small regions begin to look like circles the smaller they get.

<br>

## Fisher Information

Heuristically, the Fisher information matrix measures how much information a random variable $X$ carries about an unknown parameter $\theta$. To illustrate the intuitive idea, let's consider two extreme examples:

1) Suppose we flip a coin 100 times. Let $X$ be the number of heads we observe and $\theta$ the probability of getting heads. Then $X$ carries a lot of information about the possible value of $\theta$. Consequently $X$ should have a high Fisher information.

2) Suppose we have two coins: Coin A and Coin B. We flip Coin A 100 times and let $X$ be the number of heads we observe. Let $\theta$ be the probability of getting heads on Coin B. In this case $X$ is completely independent from $\theta$, so $X$ does not carry any information about $\theta$. Consequently $X$ should have a low Fisher information.

An equivalent and more intuitive way to describe Fisher information is via curvature. When estimating the value of $\theta$, we generally pick the value of $\theta$ which maximizes the (log) likelihood:

$$
\theta = \max_{\theta} \log L(\theta|X) = \max_{\theta} \log p(X|\theta)
$$

This choice of $\theta$ is called the **maximum likelihood estimate** (MLE) of $\theta$. Although the MLE has the largest likelihood, it's often the case that the MLE is only *slightly* larger than every other value. Intuitively this means that there is a wide range of values of $\theta$ that are nearly as plausible as the MLE. Consequently, this means $X$ doesn't really tell us much about the true value of the $\theta$. On the other hand, if the MLE is head-and-shoulders above all other values, then $X$ is telling us a lot about the true value of $\theta$. 

This idea can be quantified by looking at the *curvature* of the (log) likelihood function around the MLE. The more "flat" the log-likelihood is around the MLE, the less information we have. The more "curvature" we have around the MLE, the more information we have. Fisher information can thus be defined as **the expected curvature (aka expected 2nd derivative) at the MLE point of the (negative) log-likelihood averaged across all possible outcomes of $X$**. When we have a random vector $X$ and a vector of parameters $\theta$, the 2nd derivative is given by the Hessian matrix. Consequently, we can approximate the Fisher information matrix by approximating an "average" Hessian matrix of the negative log-likelihood.

$$
I(\theta_0) = -H(\theta_0) = -\left[\frac{\partial^2}{\partial\theta_i \partial \theta_j}\log p(\theta)\right]
$$

<br>

## Quadratic Approximation

To perform quadratic approximation, we begin by trying to approximate the posterior with the normal distribution (which it converges to asymptotically):

$$
p(\theta|(x_1,\ldots,x_n)) \approx N(\theta_0, n^{-1}I(\theta_0)^{-1})
$$

Where the mean of the normal distribution is some value $\theta_0$ and the covariance matrix is equal to the inverse of the Fisher information matrix $I(\theta_0)^{-1}$ (scaled by the number of observations $n$). Our objective then is to find good estimates for $\theta_0$ and the Fisher information matrix $I(\theta_0)^{-1}$, thereby getting a normal distribution which approximates $p(\theta|X)$.

For a normal distribution, the mean $\theta_0$ is also the mode. Since we are stipulating that the posterior $p(\theta|X)$ is approximately normal, it stands to reason that the mode of $p(\theta|X)$ should be equal to $\theta_0$. That is: $\theta_0$ should be the posterior mode:

$$
\theta_0 = \max_{\theta}p(\theta|X)
$$

Therefore, we seek the maximum of the posterior distribution $p(\theta|X)$, which we can do using the `optim()` function. However, `optim()` only finds the minimum of functions, so we will need to do some sleight of hand; observe that:

$$
\theta_0 = \max_{\theta}p(\theta|X) = \min_{\theta} -p(\theta|X)
$$

In other words: the maximum of a function $f$ is equivalent to the minimum of $-f$.

The second thing we need to estimate is the Fisher information matrix $I(\theta_0)^{-1}$. As mentioned before, this can be done by getting an approximation of the Hessian of the negative log-likelihood at the maximum of the posterior $p(\theta|X)$.

$$
I(\theta_0) = -H(\theta_0) = -\left[\frac{\partial^2}{\partial\theta_i \partial \theta_j}\log p(\theta)\right]
$$

This allows us to compute $\theta_0$ and $I(\theta_0)$ which completely determine the normal approximation:

$$
p(\theta|X) \approx N(\theta_0, n^{-1}I(\theta_0)^{-1})
$$

<br>

## Example: Flipping A Coin 10 Times

Suppose we have a coin with $\theta$ the probability of getting a heads:

```{r}
set.seed(SEED)
true_min <- runif(1, min = 0.2, max = 0.5)
true_max <- true_min + runif(1, min = 0, max = 0.2)
true_theta <- runif(1, min = true_min, max = true_max)
```

We wish to determine the value of `true_theta`. Let's pretend we had no insight as to what $\theta$ could be, other than $0\leq 1\leq \theta$. A reasonable prior in this case would be the uniform distribution on $[0,1]$

$$
p(\theta) = \text{dunif(0,1)} = 1 \quad \text{(prior)}\\
$$

To determine the value of $\theta$, we will flip the coin 10 times. Let $X$ be the number of heads we observe. Then the values of $X$ will be generated according to a binomial distribution:

$$
p(X = k|\theta) = {10 \choose k } \theta^k(1-\theta)^{10-k} = \text{dbinom}(k, 10, \theta) \qquad \text{(likelihood)}
$$

We flip the coin 10 times and observe the following results:

```{r}
set.seed(SEED)
flip_results <- rbinom(10, size = 1, prob = true_theta)

flip_results
```

Consequently, the value of $X$ (the number of heads) is:

```{r}
X_observed <- sum(flip_results)
X_observed
```

Now, we wish to compute the posterior using quadratic approximation. We thus stipulate that:

$$
p(\theta|X = 4) \approx N(\theta_0, n^{-1}I(\theta_0)^{-1})
$$

Note that since we only have 1 parameter, the "curvature" is given by a single number: the second derivative at the max $\theta_0$.

To do this, we need to define the negative log-posterior and have the `optim()` function find its minimum and Hessian:

$$
\begin{align*}
-\log p(\theta|X) &\propto -\log p(\theta) - \log p(X = 4|\theta) \\
&= -\log(1) - \log(\text{dbinom}(4, 10, \theta)) \\
&= -\log(\text{dbinom}(4, 10, \theta))
\end{align*}
$$

```{r}
objective_function <- function(theta){
  return(log(dbinom(X_observed, size = 10, prob = theta)))
}

solution <- optim(
  par = 0.5,
  fn = objective_function,
  control = list(fnscale = -1),  # fnscale scalar multiplies the function
  method = "BFGS",
  hessian = TRUE
)

solution
```

Note that $-1/Hessian$ returns the *covariance matrix*. To get the standard deviation, we need to take the square-root:

```{r}
mu = solution$par
sigma = sqrt(-1/solution$hessian[1,1])

print(paste0("mu = ", mu, " and sd = ", sigma))
```

Thus the quadratic approximation of the posterior distribution is

$$
p(\theta|X = 4) \approx N\left(0.4, 0.155\right)
$$

Graphically:

```{r}
tibble(
  theta = seq(0, 1, by = 0.01),
  posterior = dnorm(theta, mean = mu, sd = sqrt(sigma_squared))
) %>% 
  ggplot(aes(x = theta, y = posterior)) + 
  geom_line()
```

As it turns out, our prior and likelihood is simple enough to where we can analytically compute the posterior. This is a good way to check the quadratic approximation against the true solution:

$$
\begin{align*}
p(\theta|X = 4) &= \frac{p(\theta)p(X = 4|\theta)}{\int p(X = 4|\theta)p(\theta)d\theta} \\
&= \frac{1 \cdot {10 \choose 4}\theta^4(1-\theta)^6}{\int_0^1 1 \cdot {10 \choose 4}\theta^4(1-\theta)^6 d\theta} \\
&= \frac{\theta^4(1-\theta)^6}{\int_0^1\theta^4(1-\theta)^6d\theta} \\
\end{align*}
$$

Where the integral ranges from 0 to 1 because the prior $p(\theta)$ is uniform on $[0,1]$ and zero every everywhere else. To solve the integral in the denominator, we expand via the binomial theorem

$$
\begin{align*}
\int_{0}^{1} \theta^4(1-\theta)^6d\theta &= \int_{0}^{1}\theta^4 \sum_{k=0}^6 {6 \choose k}1^{k}(-\theta)^{6-k} d\theta\\
&= \int_{0}^{1}\theta^4\left( \theta^6 - 6\theta^5 + 15\theta^4 - 20\theta^3 + 15\theta^2 - 6\theta + 1\right) d\theta\\
&= \int_{0}^{1}\left(\theta^{10} - 6\theta^9 + 15\theta^8 - 20\theta^7 + 15\theta^6 - 6\theta^5 + \theta^4\right) d\theta\\
&= \frac{1}{11}\theta^{11} - \frac{6}{10}\theta^{10} + \frac{15}{9}\theta^9 - \frac{20}{8}\theta^8 + \frac{15}{7}\theta^7 -\frac{6}{6}\theta^6 +\frac{1}{5}\theta^5 |_{\theta = 0}^{\theta = 1} \\
&= \frac{1}{11} - \frac{6}{10} + \frac{15}{9} - \frac{20}{8} + \frac{15}{7} - 1 +\frac{1}{5} \\
&\approx 0.0004329004
\end{align*}
$$

Thus:

```{r}
true_posterior_distribution <- function(theta){
  return( ifelse(theta >= 0 & theta <= 1, (theta^4)*((1-theta)^6) / 0.0004329004, 0) )
}

tibble(
  theta = seq(0, 1, by = 0.01),
  posterior = true_posterior_distribution(theta),
  approx_posterior = dnorm(theta, mean = mu, sd = sqrt(sigma_squared))
) %>% 
  ggplot(aes(theta, posterior)) + 
  geom_line() + 
  geom_line(aes(y = approx_posterior), linetype = "dashed", color = "blue")
```

<br>

---

<br>

# 2.3 The `rethinking` Library

The `rethinking` library is a library of functions used to conduct Bayesian analysis. This package was developed as part of the Statistical Rethinking course and textbook by Richard McElreath. We can leverage some of the functions in this library to perform posterior computations.

To perform quadratic approximation of the posterior, we first specify a probability model. In the `rethinking` library, a "model" is defined to be an `alist()` of formulas:

```{r}
coin_flip_model <- alist(
  X ~ dbinom(10, theta),
  theta ~ dunif(0, 1)
)
```

To perform the quadratic approximation, we call the `quap()` function and pass to it the model and the observed value(s) of $X$

```{r}
library(rethinking)

# fit the quadratic approximation of the posterior
coin_flip_posterior <- rethinking::quap(
  flist = coin_flip_model,
  data = list(X = X_observed)
)

# view the estimated normal distribution
rethinking::precis(coin_flip_posterior)
```

The `quap()` function returns an approximation of:

$$
p(\theta|X) = N(0.4, 0.15)
$$
which is identical to our hand-coded approach using `optim()`. This is because `quap()` is essentially a wrap around the `optim()` function that reads the formulas and prepares the necessary backend code on our behalf!
